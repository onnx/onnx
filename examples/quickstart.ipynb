{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Quick Start Guide\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/onnx/onnx/blob/main/examples/quickstart.ipynb)\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Create a simple PyTorch model\n",
    "2. Export it to ONNX format\n",
    "3. Validate the ONNX model\n",
    "4. Run inference using ONNX Runtime\n",
    "5. Visualize the model graph\n",
    "\n",
    "**Total time: ~5 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"pip\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q onnx torch onnxruntime numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a PyTorch Model\n",
    "\n",
    "Let's create a simple convolutional neural network for image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleCNN()\n",
    "model.eval()\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export PyTorch Model to ONNX\n",
    "\n",
    "Now we'll export the model to ONNX format. ONNX uses a static graph, so we need to provide example inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input (batch_size=1, channels=3, height=32, width=32)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_model_path = \"simple_cnn.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                          # PyTorch model\n",
    "    dummy_input,                    # Example input\n",
    "    onnx_model_path,                # Output file path\n",
    "    export_params=True,             # Store trained parameters\n",
    "    opset_version=17,               # ONNX opset version\n",
    "    do_constant_folding=True,       # Optimize constant folding\n",
    "    input_names=['input'],          # Input tensor name\n",
    "    output_names=['output'],        # Output tensor name\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},  # Variable batch size\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model exported to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate the ONNX Model\n",
    "\n",
    "Let's verify that the exported model is valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Check the model\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"‚úÖ ONNX model is valid!\")\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(f\"‚ùå Model validation failed: {e}\")\n",
    "\n",
    "# Print model information\n",
    "print(f\"\\nModel IR version: {onnx_model.ir_version}\")\n",
    "print(f\"Opset version: {onnx_model.opset_import[0].version}\")\n",
    "print(f\"Producer: {onnx_model.producer_name}\")\n",
    "print(f\"\\nNumber of nodes: {len(onnx_model.graph.node)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Model Graph\n",
    "\n",
    "Let's examine the model's inputs, outputs, and operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print inputs\n",
    "print(\"Model Inputs:\")\n",
    "for input_tensor in onnx_model.graph.input:\n",
    "    print(f\"  - {input_tensor.name}: {[d.dim_value if d.dim_value != 0 else 'dynamic' for d in input_tensor.type.tensor_type.shape.dim]}\")\n",
    "\n",
    "# Print outputs\n",
    "print(\"\\nModel Outputs:\")\n",
    "for output_tensor in onnx_model.graph.output:\n",
    "    print(f\"  - {output_tensor.name}: {[d.dim_value if d.dim_value != 0 else 'dynamic' for d in output_tensor.type.tensor_type.shape.dim]}\")\n",
    "\n",
    "# Count operators\n",
    "from collections import Counter\n",
    "op_types = Counter([node.op_type for node in onnx_model.graph.node])\n",
    "print(\"\\nOperator Types:\")\n",
    "for op_type, count in op_types.most_common():\n",
    "    print(f\"  - {op_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Inference with ONNX Runtime\n",
    "\n",
    "Now let's run inference using the ONNX Runtime for optimized performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ONNX Runtime session\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Get input name\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = ort_session.get_outputs()[0].name\n",
    "\n",
    "print(f\"Input name: {input_name}\")\n",
    "print(f\"Output name: {output_name}\")\n",
    "\n",
    "# Prepare input data\n",
    "test_input = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "ort_outputs = ort_session.run([output_name], {input_name: test_input})\n",
    "\n",
    "print(f\"\\n‚úÖ Inference completed!\")\n",
    "print(f\"Output shape: {ort_outputs[0].shape}\")\n",
    "print(f\"Predicted class: {np.argmax(ort_outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare PyTorch vs ONNX Runtime\n",
    "\n",
    "Let's verify that both produce the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch inference\n",
    "with torch.no_grad():\n",
    "    torch_input = torch.from_numpy(test_input)\n",
    "    torch_output = model(torch_input).numpy()\n",
    "\n",
    "# ONNX Runtime inference\n",
    "onnx_output = ort_outputs[0]\n",
    "\n",
    "# Compare outputs\n",
    "print(\"PyTorch output:\")\n",
    "print(torch_output[0][:5])  # First 5 values\n",
    "\n",
    "print(\"\\nONNX Runtime output:\")\n",
    "print(onnx_output[0][:5])  # First 5 values\n",
    "\n",
    "# Calculate difference\n",
    "max_diff = np.abs(torch_output - onnx_output).max()\n",
    "print(f\"\\nMaximum difference: {max_diff}\")\n",
    "\n",
    "if max_diff < 1e-5:\n",
    "    print(\"‚úÖ Outputs match! The models are numerically equivalent.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Outputs differ - this might be expected due to floating point precision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmark Performance (Optional)\n",
    "\n",
    "Let's compare inference speed between PyTorch and ONNX Runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_runs = 100\n",
    "batch_input = np.random.randn(10, 3, 32, 32).astype(np.float32)\n",
    "\n",
    "# Benchmark PyTorch\n",
    "torch_input_batch = torch.from_numpy(batch_input)\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_runs):\n",
    "        _ = model(torch_input_batch)\n",
    "torch_time = (time.time() - start) / num_runs\n",
    "\n",
    "# Benchmark ONNX Runtime\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    _ = ort_session.run([output_name], {input_name: batch_input})\n",
    "onnx_time = (time.time() - start) / num_runs\n",
    "\n",
    "print(f\"PyTorch average time: {torch_time*1000:.2f} ms\")\n",
    "print(f\"ONNX Runtime average time: {onnx_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {torch_time/onnx_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize the Model (Optional)\n",
    "\n",
    "You can visualize the ONNX model graph using Netron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Colab or Jupyter, you can visualize with netron\n",
    "try:\n",
    "    import netron\n",
    "    print(\"Netron is installed. You can visualize the model at:\")\n",
    "    print(\"https://netron.app/\")\n",
    "    print(\"Or run: netron simple_cnn.onnx\")\n",
    "except ImportError:\n",
    "    print(\"To visualize the model, install netron:\")\n",
    "    print(\"  pip install netron\")\n",
    "    print(\"Then run: netron simple_cnn.onnx\")\n",
    "    print(\"\\nOr upload your model to https://netron.app/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this quick start, you learned:\n",
    "\n",
    "‚úÖ How to export a PyTorch model to ONNX format  \n",
    "‚úÖ How to validate ONNX models  \n",
    "‚úÖ How to inspect model structure and operators  \n",
    "‚úÖ How to run inference with ONNX Runtime  \n",
    "‚úÖ How to verify numerical equivalence  \n",
    "‚úÖ How to benchmark performance  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- üìñ [ONNX Documentation](https://onnx.ai/onnx/)\n",
    "- üîß [PyTorch ONNX Export Guide](https://pytorch.org/docs/stable/onnx.html)\n",
    "- üöÄ [ONNX Runtime Documentation](https://onnxruntime.ai/)\n",
    "- üí° [ONNX Model Zoo](https://github.com/onnx/models) - Pre-trained ONNX models\n",
    "- üõ†Ô∏è [ONNX Tutorials](https://github.com/onnx/tutorials) - Advanced examples\n",
    "\n",
    "## Questions or Issues?\n",
    "\n",
    "- üí¨ [ONNX GitHub Discussions](https://github.com/onnx/onnx/discussions)\n",
    "- üêõ [Report Issues](https://github.com/onnx/onnx/issues)\n",
    "- üìß [Community Slack](https://lfaifoundation.slack.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
