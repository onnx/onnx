# OpSchema
ONNX provides an OpSchema object as a general way to describe an arbitrary operation through a specification. OpSchema is is defined in `onnx/onnx/defs/schema.h`. All core operations done on OpSchema are done in place and return a reference to the OpSchema. This allows for specifications to be done in a piped matter.

Constructing a new Operator set should be done with the `ONNX_OPERATOR_SET_SCHEMA` definition. Below is an example of defining the LeakyRelu Op.

```c++
static const char* LeakyRelu_ver6_doc = R"DOC(
LeakyRelu takes input data (Tensor<T>) and an argument alpha, and produces one
output data (Tensor<T>) where the function `f(x) = alpha * x for x < 0`,
`f(x) = x for x >= 0`, is applied to the data tensor elementwise.
)DOC";

ONNX_OPERATOR_SET_SCHEMA(
    LeakyRelu, // Name
    6, // Version
    OpSchema() // Specification
        .Attr("alpha", "Coefficient of leakage.", AttributeProto::FLOAT, 0.01f)
        .SetDoc(LeakyRelu_ver6_doc)
        .Input(0, "X", "Input tensor", "T")
        .Output(0, "Y", "Output tensor", "T")
        .TypeConstraint(
            "T",
            {"tensor(float16)", "tensor(float)", "tensor(double)"},
            "Constrain input and output types to float tensors.")
        .TypeAndShapeInferenceFunction(propagateShapeAndTypeFromFirstInput)
        .AddOpAnnotation(OpAnnotationFlag::ElementwiseIndependent)
        .AddOpAnnotation(OpAnnotationFlag::ElementwiseWeakMonotonicIncreasing));
```

## Type Support
OpSchema provides us with a way to specify exactly what types are supported by your Op. This is done by the TypeConstraint function. In the example above, we define type `T` to be any tensor of floating points `(float, float16, double)`. OpSchema allows you to define multiple type constraints need be.
## Inputs and Outputs
Next we need to define the inputs and outputs needed by our model. Specification of `Input` and `Output` follows the same pattern `(index, name, description, type)`. The type used must be a specified type generated by `TypeConstraint`.
## Shape Inference
OpSchema also provides a way to describe the shape inference portion of your Op. The way to do this is to define a `std::function<void(InferenceContext&)>;` which propagates shape information to output nodes. A simple implementation of this is `propagateShapeAndTypeFromFirstInput` which maintains the same input shape for it's output.

InferenceContext provides us with the following information.
```c++
struct InferenceContext {
  virtual const AttributeProto* getAttribute(const std::string& name) const = 0;
  virtual size_t getNumInputs() const = 0;
  virtual const TypeProto* getInputType(size_t index) const = 0;
  virtual const TensorProto* getInputData(size_t index) const = 0;
  virtual size_t getNumOutputs() const = 0;
  virtual TypeProto* getOutputType(size_t index) = 0;
  virtual GraphInferencer* getGraphAttributeInferencer(
      const std::string& attribute_name) = 0;
  virtual ~InferenceContext() {}
};
```
## Annotation
Op Annotations provide us with a way to explain the general properties of the Op. These annotations are completely optional but become very useful during the ONNX optimization problem. They provide a level of generalizability to our optimization framework by allowing us not to rely on individual ops but rather high level information about how they operate. In the example above, we annotate our LeakyRelu Op as being both elementwise and weak monotonic increasing. Because of our annotation we automatically reap the benefits of the `eliminate_nop_monotone_argmax` pass which removes any monotonic Op before an argmax.

For a more detailed description of all the annotations available and their meaning, please refer to `onnx/defs/op_annotation.h`
## Documentation
It is also neccessary to provide a documentation string per Op, which is done through the SetDoc function.